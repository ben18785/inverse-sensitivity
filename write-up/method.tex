\section{Method}\label{sec:method}
In this section, we first describe a probabilistic framework that describes our inverse problem, before introducing the CMC algorithm in pseudocode (Algorithm \ref{alg:cmc}). We also detail the workflow we have found helpful in using CMC to analyse cell snapshot data and suggest practical remedies to issues commonly encountered whilst using this approach (Figure \ref{fig:workflow}). A glossary of all the variable names used in this paper is included as Table \ref{tab:variable_glossary}.

Experimental methods such as flow cytometry measure single cell characteristics at a given time. Cells are typically destroyed by the measurement process and so rather than providing time series for each individual cell, the data consists of cross-sections or ``snapshots'' of sampled individuals from the population (Figure \ref{fig:time_series_v_snapshots}).

\begin{figure}[H]
	\centerline{\includegraphics[width=\textwidth]{../figures/time_series_v_snapshots.pdf}}
	\caption{\textbf{Time series data (A) versus snapshot data (B) typical of single cell experiments.} In A, note cell identities are retained at each measurement time (indicated by given plot markers) whereas in the snapshot data in B, either this information is lost or, more often, cells are destroyed by the measurement process and so each observation corresponds to a distinct cell.}
	\label{fig:time_series_v_snapshots}
\end{figure}

We model the processes of an individual cell using a system of ordinary differential equations (ODEs), where each element of the system corresponds to a quantity of interest (for example, protein levels, RNA concentrations and so on),
%
\begin{equation}\label{eq:ode}
\begin{aligned}
\frac{d\boldsymbol{x}}{dt} &= \boldsymbol{f}(\boldsymbol{x}(t); \, \boldsymbol{\theta}), \quad \boldsymbol{f}: \R^k \times \R^p \mapsto \R^k, \\
\boldsymbol{x}(0) &= \boldsymbol{x}_0.
\end{aligned}
\end{equation}
%
Note that in most circumstances, the initial state of the system, $\boldsymbol{x}(0)$, is unknown and it is convenient to include these as elements of $\boldsymbol{\theta}$ to be estimated.


\subsection{Snapshot data}
We assume variation characterised by snapshot data arises due to between-cell heterogeneity in the underlying parameters $\boldsymbol{\theta}$. Therefore, the evolution of the underlying state of cell $i$ is described by an idiosyncratic ODE,
%
\begin{equation} \label{eq:ode_i}
\begin{aligned}
\frac{d\boldsymbol{x}^{\{i\}}}{dt} &= \boldsymbol{f} \left( \boldsymbol{x}^{\{i\}}(t); \, \boldsymbol{\theta}^{\{i\}} \right),
                                      \quad \boldsymbol{f}: \R^k \times \R^p \mapsto \R^k, \\
\boldsymbol{x}^{\{i\}}(0) &= \boldsymbol{x}_0
\end{aligned}
\end{equation}
where $^{\{i\}}$ indicates the $i$th cell.
%
The traditional (non-hierarchical) state-space approach to modelling dynamic systems supposes that measurement randomness generates output variation (Figure \ref{fig:data_generation}A). Our approach, by contrast, assumes stochasticity in outputs is solely due to variability in parameter values between cells (Figure \ref{fig:data_generation}B). Whether the assumption of ``perfect'' measurements is reasonable depends on experimental details of the system under investigation but we argue our method nevertheless provides a useful approximation in cases where the signal to noise ratio is high.

\begin{figure}[H]
	\centerline{\includegraphics[width=\textwidth]{../figures/data_generation.pdf}}
	\caption{\textbf{Two ways to generate randomness in measured outputs: the state-space model (A) versus the parameter heterogeneity model (B).} For non-hierarchical state-space models (A), there is assumed to be a single ``true'' latent state where observations result from a noisy measurement process (grey histograms). For models with parameter heterogeneity (B), the uncertainty is generated by differences in cellular processes (black lines) between cells. Note that in both cases, individual cells are measured only once in their lifetime.}
	\label{fig:data_generation}
\end{figure}

We suppose $m$ quantities of interest (QOIs) are measured,
%
\begin{equation}
\boldsymbol{q}^\top = \left( q_1, q_2, \dots, q_m \right) \in \R^m,
\end{equation}
% 
with $n_j$ observations of each quantity, $q_j$. Each QOI may correspond to different functionals of the solution at the same time or the same functional at different times or a mixture of both. The observed data for QOI $j$ at time $t_j$ then consists of the $n_j$ cellular measurements,
%
\begin{equation}
\boldsymbol{y}(t_j)^\top = \left( q_j(x^{\{1\}}(t_j)), q_j(x^{\{2\}}(t_j)), \dots, q_j(x^{\{n_j\}}(t_j))  \right) \in \R^{n_j}.
\end{equation}
%
The raw snapshot data $\boldsymbol{X}$ then consists of the collection of all measured QOIs,
%
\begin{equation}
\boldsymbol{X} = \left( \boldsymbol{y}(t_1), \boldsymbol{y}(t_2), \dots, \boldsymbol{y}(t_m) \right) \in \R^{n_1} \times \R^{n_2}\times...\times\R^{n_m}.
\end{equation}
%
The goal of inference is to characterise the probability distribution $p(\boldsymbol{\theta}|\boldsymbol{X})$ representing heterogeneity in cellular processes. Raw snapshot data consists of individual cell measurements with exact inference requiring simulations of the underlying ODE system for each individual. This is cumbersome and impractical for the numbers of cells sampled in typical experimental setups and so, instead, we follow previous work and represent snapshot data $\boldsymbol{X}$ using probability distributions \cite{hasenauer2011identification,hasenauer2014ode,loos2018hierarchical,dixit2018maximum}. In the first step of our workflow (Figure \ref{fig:workflow}(i)), these distributions are approximated by a kernel density model, with support over the space of the QOI vector $\boldsymbol{q}\in\R^m$. We denote $\hat{\Phi}$ as the parameter estimates of the corresponding kernel density model $p(\boldsymbol{q}|\Phi)$ fitted to the raw snapshot data. We assume that the volume of observational data means the estimated probability distributions are approximate sufficient statistics of the outputs, meaning $p(\boldsymbol{\theta}|\hat{\Phi}) \approx p(\boldsymbol{\theta}|\boldsymbol{X})$.

\begin{table}[htbp]
\centering
%\scriptsize
\begin{adjustwidth}{-0.5in}{0in}%
\begin{tabularx}{1.2\textwidth}{lll}
Variable	                                                & Definition                                   & Dimension \\
\toprule
$\boldsymbol{x}(t)$                                     	& ODE solution                                 & $\R^k$ \\
$\boldsymbol{\theta}$                                     	& ODE parameters                               & $\R^p$ \\
$\boldsymbol{f}(\boldsymbol{x}(t); \, \boldsymbol{\theta})$	& ODE RHS                                      & $\R^k$ \\
$\boldsymbol{x}^{\{i\}}(t)$                                 & ODE solution for cell $i$                    & $\R^k$ \\
$q_j= q_j(\boldsymbol{x}(t_j);\boldsymbol{\theta}) = q_j(\boldsymbol{\theta})$                             & quantity of interest (QOI) $j$               & $\R^1$ \\
$\boldsymbol{q}^\top= \left( q_1, \dots, q_m \right)$            & $m$ distinct QOIs                            & $\R^m$ \\
$q_j^{\{i\}}= q_j(\boldsymbol{x}^{\{i\}}(t_j))$             & QOI $j$ for cell $i$                         & $\R^1$ \\
$\boldsymbol{y}_j^\top=\left( q_j^{\{1\}}, \dots q_j^{\{n_j\}} \right)$  & QOI $j$ for cells $1, \dots, n_j$    & $\R^{n_j}$ \\
$\boldsymbol{X}=(\boldsymbol{y}_1,...,\boldsymbol{y}_m)$    & ``snapshot'' of all QOIs                      & $\R^{n_1} \times \R^{n_2}\times...\times\R^{n_m}$ \\
$\Phi$ & parametrises output target distribution $p(\boldsymbol{q}|\Phi)$              & $\R^m$ \\
$\Xi$  & parametrises prior parameter distribution $p(\boldsymbol{\theta}|\Xi)$        & $\R^p$ \\
$\Psi$ & parametrises prior output distribution $p(\boldsymbol{q}|\Psi)$               & $\R^p$ \\
$\hat{a}$ & estimates of any quantity $a$                                                                  & - \\
$\Omega(\boldsymbol{z})$              & parameter space mapping to $\boldsymbol{q}=\boldsymbol{z}$         & $\R^{\leq p}$ \\
$\mathcal{V}(\boldsymbol{z})$         & volume of $\Omega(\boldsymbol{z})$                                 & $\R^+$ \\
$V$                                   & volume of (bounded) parameter space                                          & $\R^+$ \\
\end{tabularx}
\caption{\textbf{Glossary of variable names used in this paper.}}
\label{tab:variable_glossary}
\end{adjustwidth}
\end{table}




\subsection{Theoretical development of CMC}
We consider the underdetermined case where $m<p$, so that each quantity of interest $\tilde{\boldsymbol{q}}$ can be generated from a non-singular set of parameter values, which we term iso-output contour regions: $\Omega(\tilde{\boldsymbol{q}}) = \{\boldsymbol{\theta}: \boldsymbol{q}(\boldsymbol{\theta}) = \tilde{\boldsymbol{q}}\}$. In general, these contours have ``volumes'' $\mathcal{V}(\tilde{\boldsymbol{q}})$ which depend on the chosen output value $\tilde{\boldsymbol{q}}$ (Figure \ref{fig:contour_volumes}). Any algorithm which samples parameter values in order to generate a given output target must account for the differential volumes of these sets, otherwise sampling will be biased towards iso-output contours with larger volumes \cite{lambert2018inverse}. The problem in most applied problems however is that we do not know \textit{a priori} the volumes of iso-output contours and so they must be estimated. The following analysis provides a brief introduction to a probabilistic formulation of underdetermined inverse problems (see our companion paper \cite{lambert2018inverse} for a more comprehensive discussion) and, in doing so, suggests a sampling approach for estimating the volumes of parameter space mapping to each output value, which forms the basis of CMC.

\begin{figure}[H]
	\centerline{\includegraphics[width=\textwidth]{../figures/contour_volumes_redux.pdf}}
	\caption{\textbf{Left: An example output function $q(\theta_1,\theta_2)$ along with iso-output contours indicated (coloured lines) . Right: The ``volume'' of output contours as a function of output value.} Note that here, since the input space is two dimensional, the ``volume'' of each output value corresponds to a length of an iso-output contour.}
	\label{fig:contour_volumes}
\end{figure}

Solving our inverse problem equates to determining the posterior distribution of parameter values $p(\boldsymbol{\theta}|\hat{\Phi})$  which, when inputted to the forward map, results in the target distribution $p(\boldsymbol{q}|\hat{\Phi})$. To derive the posterior parameter distribution, we consider the joint density of parameters and QOIs $p(\boldsymbol{\theta},\boldsymbol{q}|\hat{\Phi})$. This can be decomposed in two ways,
%
\begin{equation}\label{eq:joint}
  p( \boldsymbol{\theta}, \boldsymbol{q}|\hat{\Phi})
= p( \boldsymbol{\theta}|\boldsymbol{q}, \hat{\Phi}) \times p(\boldsymbol{q}|\hat{\Phi})
= p( \boldsymbol{q}|\boldsymbol{\theta}, \hat{\Phi} ) \times p(\boldsymbol{\theta}|\hat{\Phi}).
\end{equation}
%
Rearranging to obtain the posterior parameter distribution,
%
\begin{equation}\label{eq:posterior_mid}
p(\boldsymbol{\theta}|\hat{\Phi})
= \frac{p(\boldsymbol{\theta}|\boldsymbol{q}, \hat{\Phi}) \times p(\boldsymbol{q}|\hat{\Phi})}{p(\boldsymbol{q}| \boldsymbol{\theta}, \hat{\Phi})}.
\end{equation}
%
Given parameters $\boldsymbol{\theta}$, the mapping from parameters to outputs is deterministic meaning
$p(\boldsymbol{q}| \boldsymbol{\theta}, \hat{\Phi})=\delta(\boldsymbol{q}(\boldsymbol{\theta}))$ is the Dirac delta function centred at $\boldsymbol{q}=\boldsymbol{q}(\boldsymbol{\theta})$, meaning all $\boldsymbol{q}\rightarrow \boldsymbol{q}(\boldsymbol{\theta})$ in the numerator of eq. (\ref{eq:posterior_mid}),
%
\begin{equation}\label{eq:posterior_mid1}
p(\boldsymbol{\theta}|\hat{\Phi})
= p(\boldsymbol{\theta}|\boldsymbol{q}(\boldsymbol{\theta}), \hat{\Phi}) \times p(\boldsymbol{q}(\boldsymbol{\theta})|\hat{\Phi}).
\end{equation}
%
In the same way that a single output value can be caused by any of a set of parameter values, a target output distribution $p(\boldsymbol{q}|\hat{\Phi})$ can be caused by any members of a set of parameter distributions. To ensure uniqueness of the ``posterior'' parameter distributions, our probabilistic framework therefore requires that we specify ``prior'' distributions for the parameters, as in more traditional Bayesian inference. In what follows, we assume the conditional distribution $p(\boldsymbol{\theta}|\boldsymbol{q}, \hat{\Phi})=p(\boldsymbol{\theta}|\boldsymbol{q})$ is independent of the data, meaning it represents a conditional ``prior'', which can be manipulated using Bayes' rule,
%
\begin{equation}\label{eq:prior}
p(\boldsymbol{\theta}|\boldsymbol{q}(\boldsymbol{\theta})) = \frac{p(\boldsymbol{\theta})}{p(\boldsymbol{q}(\boldsymbol{\theta}))}.
\end{equation}
%
This results in the form of the posterior parameter distribution targeted by our sampling algorithm,
%
\begin{equation}\label{eq:posterior_input}
p(\boldsymbol{\theta}|\hat{\Phi}) = \frac{p(\boldsymbol{\theta})}{p(\boldsymbol{q}(\boldsymbol{\theta}))} p(\boldsymbol{q}(\boldsymbol{\theta})|\hat{\Phi}).
\end{equation}
%
Again, we defer to our companion piece \cite{lambert2018inverse} for detailed explanation of eqs. (\ref{eq:prior}) \& (\ref{eq:posterior_input}) and instead here provide brief interpretation when considering a uniform prior on parameter space. In this case, $p(\boldsymbol{\theta}) = \frac{1}{V}$, where $V$ is the total volume of parameter space. The denominator term of eq. (\ref{eq:prior}) is the prior induced on output space by the prior over parameter space. For a uniform prior on parameter values, this is,
%
\begin{equation}\label{eq:contour_volume}
p(\boldsymbol{\theta}|\boldsymbol{q}(\boldsymbol{\theta})) = \frac{1}{\mathcal{V}(\boldsymbol{q}(\boldsymbol{\theta}))},
\end{equation}
%
where $\mathcal{V}(\boldsymbol{q}(\boldsymbol{\theta}))$ is the volume of parameter space occupied by the iso-output contour $\Omega(\boldsymbol{q}(\boldsymbol{\theta}))$. Therefore a uniform prior over parameter space implies a prior structure where all parameter values producing the same output are given equal weighting.

\subsection{Implementation of CMC}

Except for some toy examples, the denominator of eq. (\ref{eq:prior}) cannot be calculated, and exact sampling from the posterior parameter distribution of eq. (\ref{eq:posterior_input}) is not, in general, possible. We propose instead a computationally efficient sampling method to estimate $p(\boldsymbol{q}(\boldsymbol{\theta}))$, which forms the first step of our so-called ``Contour Monte Carlo'' (CMC) algorithm (Algorithm \ref{alg:cmc}; Figure \ref{fig:workflow}(ii)), where we estimate the volume of iso-output contours with output value $\boldsymbol{q}(\boldsymbol{\theta})$. This step involves repeated independent sampling from the prior distribution of parameters $\boldsymbol{\theta}^{\{i\}}\sim p(\boldsymbol{\theta}|\Xi)$, where, for completeness, we have conditioned on $\Xi$ parameterising the prior probability density. Each parameter sample is then mapped to an output value $\boldsymbol{q}^{\{i\}}=\boldsymbol{q}(\boldsymbol{\theta}^{\{i\}})$. The collection of output samples is then fitted using a vine copula kernel density estimator (KDE) \cite{nagler2016evading}, $(\boldsymbol{q}^{\{1\}},...,\boldsymbol{q}^{\{N_1\}})\sim p({\boldsymbol{q}}|\hat{\Psi})$. Throughout the course of development of CMC, we have tested many KDE methods and found vine copula KDE is best suited to approximating the higher dimensional probability distributions required in practice.


The second step in our algorithm then uses Markov chain Monte Carlo (MCMC) to sample from an approximate version of eq. (\ref{eq:posterior_input}) with the estimated density $p(\boldsymbol{q}(\boldsymbol{\theta})|\hat{\Psi})$ replacing its corresponding estimand (Algorithm \ref{alg:cmc}; Figure \ref{fig:workflow}(iii)),
%
\begin{equation}\label{eq:posterior_input_estimated}
p(\boldsymbol{\theta}|\hat{\Phi},\Xi,\hat{\Psi}) =
\frac{p(\boldsymbol{\theta}|\Xi)}{p(\boldsymbol{q}(\boldsymbol{\theta})|\hat{\Psi})} \,
p(\boldsymbol{q}(\boldsymbol{\theta})|\hat{\Phi}).
\end{equation}
%
The final step in CMC is to compare output samples generated by MCMC with the target distribution (Figure \ref{fig:workflow}(iv)). Asymptotically (in terms of the sample size of both sampling steps), CMC produces a sample of parameter values $(\boldsymbol{\theta}^{\{1\}},\boldsymbol{\theta}^{\{2\}},...)$ which, when mapped to the output space, corresponds to the target distribution $p(\boldsymbol{q}|\hat{\Psi})$. In developing CMC, we find a finite sample of modest size for both steps of CMC results in parameter samples that, when transformed, often represent reasonable approximations of the target. There are however occasions when this is not the case and this final confirmatory step is indispensable since it frequently highlights inadequacies in contour volume estimation or MCMC, meaning more samples from either or both of these steps are required. It may also be necessary to tweak hyperparameters of the KDE to ensure reasonable approximation in the contour volume estimation step. If the target distribution is sensitive to the contour volume estimates, this may also indicate that the target snapshot distribution is incompatible with the model: here, we make no claims on existence of a solution to the inverse problem, only that, if one should exist, Contour Monte Carlo is a pragmatic approach to approximate it by sampling. A useful way to diagnose whether the target distribution can be produced from the model and specified priors is to examine the output values from the contour volume estimation step of CMC. If the majority of probability mass of the target lies outside the bounds of the bulk simulated output values obtained by independent sampling from the prior, then the model and/or chosen prior is unlikely to be invertible to this particular target.

\subsection{Workflow and CMC algorithm}

A graphical illustration of the complete CMC workflow is provided in Figure \ref{fig:workflow}. All variables are defined in Table \ref{tab:variable_glossary}. The CMC algorithm is provided in Algorithm \ref{alg:cmc}. For simplicity, in this implementation MCMC sampling is performed via the Random Walk Metropolis algorithm, but for the examples in \S \ref{sec:results}, we use an adaptive MCMC algorithm \cite{johnstone2016uncertainty}.

\begin{figure}[H]
\centerline{\includegraphics[width=1.5\textwidth]{../figures/workflow.pdf}}
\caption{\textbf{The workflow for using Contour Monte Carlo to estimate cell population heterogeneity.}
The distribution targeted in (iii) is given by eq. (\ref{eq:posterior_input_estimated}).}\label{fig:workflow}
\end{figure}

\begin{algorithm}[H]
\footnotesize
\texttt{\\}
\begin{algorithmic}
\Procedure{CMC}{$\boldsymbol{X}, \Xi, N_1, N_2$}\Comment{Sample from posterior parameter distribution}
	\State $\hat{\Phi} = \Call{SnapshotEstimator}{\boldsymbol{X}}$
	\State $\hat{\Psi} = \Call{ContourVolumeEstimator}{\Xi,N_1}$
	\State $\left(\boldsymbol{\theta}^{\{1\}},...,\boldsymbol{\theta}^{\{N_2\}}\right) = \Call{MCMC}{\hat{\Phi},\Xi, \hat{\Psi},N_2}$
	\State $\text{converged} = \Call{CompareOutputToTarget}{(\boldsymbol{\theta}^{\{1\}},...,\boldsymbol{\theta}^{\{N_2\}}), \hat{\Phi}}$
	\While{converged$\neq$1} \Comment{Rerun contour volume estimation (if necessary modify vine copula KDE hyperparmeters) and/or MCMC, with larger sample sizes if required}
		 \State $\hat{\Psi}=\Call{ContourVolumeEstimator}{\Xi, N_1'},\; N_1' \geq N_1$
         \State $\left( \boldsymbol{\theta}^{\{1\}},...,\boldsymbol{\theta}^{\{N_2'\}} \right)
                = \Call{MCMC}{\hat{\Phi},\Xi, \hat{\Psi},N_2'}, \; N_2' \geq N_2$
          \State $\text{converged} = \Call{CompareOutputToTarget}{(\boldsymbol{\theta}^{\{1\}},...,\boldsymbol{\theta}^{\{N_2'\}} ), \hat{\Phi}}$
          \State $N_1 \leftarrow N_1', \; N_2 \leftarrow N_2'$
	\EndWhile
	\State \Return $\left( \boldsymbol{\theta}^{\{1\}},...,\boldsymbol{\theta}^{\{N_2\}} \right)$
\EndProcedure
\end{algorithmic}

\texttt{\\}
\begin{algorithmic}
\Procedure{SnapshotEstimator}{$\boldsymbol{X}$}\Comment{Fit snapshots with kernel density estimator (KDE)}
	\State $\boldsymbol{X} \sim p(\boldsymbol{q}|\hat{\Phi})$
	\State \Return $\hat{\Phi}$
\EndProcedure
\end{algorithmic}
	
\texttt{\\}
\begin{algorithmic}
\Procedure{ContourVolumeEstimator}{$\Xi, N_1$}\Comment{Estimate volume of contours}
	\For{$i$ in $1:N_1$}
		\State $\boldsymbol{\theta}^{\{i\}} \sim p(\boldsymbol{\theta}|\Xi)$           \Comment{Sample from prior density}
		\State $\boldsymbol{q}^{\{i\}} = \boldsymbol{q}(\boldsymbol{\theta}^{\{i\}})$  \Comment{Calculate corresponding output value}
	\EndFor
	\State $\left( \boldsymbol{q}^{\{1\}}, \dots ,\boldsymbol{q}^{\{N_1\}} \right) \sim p(\boldsymbol{q}|\hat{\Psi})$
           \Comment{Fit vine copula KDE}
	\State \Return $\hat{\Psi}$
\EndProcedure
\end{algorithmic}

\texttt{\\}
\begin{algorithmic}
\Procedure{MCMC}{$\hat{\Phi},\Xi, \hat{\Psi}, N_2$}\Comment{Random Walk Metropolis algorithm targeting posterior parameter distribution}
	\State $\boldsymbol{\theta}^{\{0\}} \sim \pi(.)$ \Comment{Sample from arbitrary initialisation distribution}
	\For{$i$ in $1:N_2$}
		\State $\boldsymbol{\theta}^{\{i\}^\prime}\sim \mathcal{N}(\boldsymbol{\theta}^{\{i-1\}},\boldsymbol{\Sigma})$ \Comment{Propose new parameter values}
		\State
		\Comment{Calculate Metropolis acceptance ratio}
		\State $r = 
                     p(\boldsymbol{\theta}^{\{i\}^\prime}|\Xi) \
                     p(\boldsymbol{q}(\boldsymbol{\theta}^{\{i-1\}})|\hat{\Psi}) \
                     p(\boldsymbol{q}(\boldsymbol{\theta}^{\{i\}^\prime})|\hat{\Phi})
                     /
                     \left[
                     p(\boldsymbol{\theta}^{\{i-1\}}|\Xi) \
                     p(\boldsymbol{q}(\boldsymbol{\theta}^{\{i\}^\prime})|\hat{\Psi}) \
                     p(\boldsymbol{q}(\boldsymbol{\theta}^{\{i-1\}})|\hat{\Phi})
                     \right]$
		\State $u\sim U(0,1)$ \Comment{Sample from uniform distribution}
		\If{$r > u$}
		    \State $\boldsymbol{\theta}^{\{i\}} = \boldsymbol{\theta}^{\{i\}^\prime}$ \Comment{Accept proposal}
		\Else
		    \State $\boldsymbol{\theta}^{\{i\}} = \boldsymbol{\theta}^{\{i-1\}}$ \Comment{Reject proposal}
		\EndIf
	\EndFor
	\State \Return $\left( \boldsymbol{\theta}^{\{1\}},...,\boldsymbol{\theta}^{\{N_2\}} \right)$
\EndProcedure
\end{algorithmic}

\texttt{\\}
\begin{algorithmic}
\Procedure{CompareOutputToTarget}{$(\boldsymbol{\theta}^{\{1\}},...,\boldsymbol{\theta}^{\{N_2\}}), \hat{\Phi}$}\Comment{Check output distribution close to target}
	\For{$i$ in $1:N_2$}
		\State $\tilde{\boldsymbol{q}}_i = \boldsymbol{q}(\boldsymbol{\theta}^{\{i\}})$ \Comment{Compute QOIs for each parameter sample}
	\EndFor
	\If{$ p(\tilde{\boldsymbol{q}}) \approx p(\tilde{\boldsymbol{q}}|\hat{\Phi})?$} \Comment{Compare sampled output distribution with target}
		\State \Return 1 \Comment{If sufficiently close then converged}
	\Else
		\State \Return 0
	\EndIf
\EndProcedure
\end{algorithmic}

\caption{Pseudocode for the Contour Monte Carlo algorithm for sampling from the posterior parameter distribution of eq. (\ref{eq:posterior_input_estimated}). }\label{alg:cmc}
\end{algorithm}

In generating our results in \S\ref{sec:results}, for the contour volume estimation step, we assumed sample sizes were sufficient if the output samples from the MCMC provided a reasonable approximation to the target, although we recognise that future work should refine this process further. For the MCMC step, we used adaptive covariance MCMC (see SOM of \cite{johnstone2016uncertainty}) to sample from the target distribution, as we have found that it provides a considerable speed-up over Random Walk Metropolis \cite{metropolis1953equation,lambert2018Student}. We also use the Gelman-Rubin convergence statistic $\hat{R}$ which provides a heuristic measurement of convergence \cite{lambert2018Student,gelman1992inference}, and use a threshold of $\hat{R}\leq\sim 1.1$ to diagnose convergence.

To solve the forward model of each differential equation, we used Julia's inbuilt ``solve'' method for ODE models, which automatically chooses an efficient inbuilt solver \cite{bezanson2017julia}.
